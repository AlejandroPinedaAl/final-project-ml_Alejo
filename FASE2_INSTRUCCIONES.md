# üìã FASE 2: INGENIER√çA DE CARACTER√çSTICAS - INSTRUCCIONES

## ‚úÖ LIBRER√çAS NECESARIAS

### Librer√≠as ya instaladas (de la Fase 1):
- ‚úÖ pandas
- ‚úÖ numpy
- ‚úÖ matplotlib
- ‚úÖ seaborn
- ‚úÖ scipy

### Librer√≠as a instalar para la Fase 2:

```bash
pip install scikit-learn joblib
```

**Nota**: Si ya las instalaste, puedes verificar con:
```bash
pip list | findstr "scikit-learn joblib"
```

---

## üì¶ DEPENDENCIAS COMPLETAS

### Comandos de instalaci√≥n:

```bash
# Instalar scikit-learn (incluye todas las herramientas de ML)
pip install scikit-learn

# Instalar joblib (para guardar/cargar modelos)
pip install joblib
```

### Verificar instalaci√≥n:

```bash
python -c "import sklearn; print('scikit-learn:', sklearn.__version__)"
python -c "import joblib; print('joblib: OK')"
```

---

## üìì NOTEBOOK CREADO

**Archivo**: `mlops_pipeline/src/feature_engineering_fase2.ipynb`

Este notebook contiene todas las celdas necesarias para ejecutar la Fase 2 completa.

---

## üöÄ PASOS PARA EJECUTAR LA FASE 2

### 1. Verificar que la Fase 1 est√© completada

Aseg√∫rate de que tengas el archivo `data_with_features.csv` en la ra√≠z del proyecto:
- Si ejecutaste la Fase 1 manualmente, deber√≠as tener este archivo
- Si no, el notebook cargar√° desde `Base_de_datos.csv` autom√°ticamente

### 2. Abrir el notebook

```bash
# Desde Jupyter Notebook o JupyterLab
jupyter notebook mlops_pipeline/src/feature_engineering_fase2.ipynb
```

O desde VS Code:
- Abre el archivo `feature_engineering_fase2.ipynb`
- Selecciona el kernel de Python

### 3. Ejecutar las celdas en orden

El notebook est√° organizado en las siguientes secciones:

1. **Importaci√≥n de Librer√≠as** (Celda 1)
2. **Carga de Datos** (Celdas 2-3)
3. **Limpieza de Datos** (Celdas 4-8)
   - Eliminar variables irrelevantes
   - Convertir tipos de datos
   - Manejo de valores nulos
   - Unificaci√≥n de categor√≠as
4. **Creaci√≥n de Features Derivados** (Celdas 9-11)
   - Features de gastos y compras
   - Features de campa√±as
   - Features temporales
5. **Preparaci√≥n para Modelado** (Celdas 12-14)
   - Separar X e y
   - Identificar tipos de variables
6. **Pipeline de Preprocesamiento** (Celdas 15-17)
   - Pipeline num√©rico
   - Pipeline categ√≥rico
   - Combinar pipelines
7. **Split de Datos** (Celdas 18-21)
   - Dividir train/test
   - Transformar datos
   - Obtener nombres de features
8. **Guardar Resultados** (Celdas 22-24)
   - Guardar preprocessor
   - Guardar datos procesados
   - Guardar datos transformados
9. **Resumen Final** (Celda 25)

### 4. Verificar resultados

Despu√©s de ejecutar todas las celdas, deber√≠as tener los siguientes archivos:

- ‚úÖ `preprocessor.pkl` - Pipeline de preprocesamiento
- ‚úÖ `data_processed.csv` - Dataset procesado completo
- ‚úÖ `X_train_transformed.csv` - Features de entrenamiento transformadas
- ‚úÖ `X_test_transformed.csv` - Features de prueba transformadas
- ‚úÖ `y_train.csv` - Target de entrenamiento
- ‚úÖ `y_test.csv` - Target de prueba

---

## üîç VERIFICACI√ìN DE RESULTADOS

### Verificar que los archivos se crearon:

```bash
# Desde la ra√≠z del proyecto
dir preprocessor.pkl
dir data_processed.csv
dir X_train_transformed.csv
dir X_test_transformed.csv
dir y_train.csv
dir y_test.csv
```

### Verificar dimensiones:

El notebook imprime un resumen al final con:
- Dimensiones del dataset procesado
- N√∫mero de features derivados
- N√∫mero de variables num√©ricas y categ√≥ricas
- Dimensiones de train y test sets

---

## ‚ö†Ô∏è POSIBLES PROBLEMAS Y SOLUCIONES

### Problema 1: Error al cargar datos

**Soluci√≥n**: Verifica que el archivo `Base_de_datos.csv` o `data_with_features.csv` exista en la ra√≠z del proyecto.

### Problema 2: Error con OneHotEncoder

**Soluci√≥n**: Aseg√∫rate de tener scikit-learn >= 1.2.0 instalado:
```bash
pip install --upgrade scikit-learn
```

### Problema 3: Error al guardar preprocessor

**Soluci√≥n**: Verifica que joblib est√© instalado:
```bash
pip install joblib
```

### Problema 4: Warning sobre sparse_output

**Soluci√≥n**: Si usas scikit-learn < 1.2, cambia `sparse_output=False` a `sparse=False` en el notebook.

---

## üìä ESTRUCTURA DEL NOTEBOOK

El notebook est√° organizado de la siguiente manera:

```
1. Introducci√≥n y Objetivo
2. Importaci√≥n de Librer√≠as
3. Carga de Datos
4. Limpieza de Datos
   - Eliminar variables irrelevantes
   - Convertir tipos
   - Manejar nulos
   - Unificar categor√≠as
5. Creaci√≥n de Features Derivados
   - 12 features nuevos
6. Preparaci√≥n para Modelado
   - Separar X e y
   - Identificar tipos
7. Pipeline de Preprocesamiento
   - Pipelines num√©rico y categ√≥rico
   - ColumnTransformer
8. Split de Datos
   - Train/Test estratificado
   - Transformaci√≥n
9. Guardar Resultados
   - Preprocessor
   - Datos procesados
10. Resumen Final
```

---

## ‚úÖ CHECKLIST DE VERIFICACI√ìN

Antes de pasar a la Fase 3, verifica que:

- [ ] Todas las celdas del notebook se ejecutaron sin errores
- [ ] El archivo `preprocessor.pkl` se cre√≥ correctamente
- [ ] Los archivos CSV se guardaron correctamente
- [ ] Las dimensiones de train y test son correctas
- [ ] La distribuci√≥n de clases se mantiene en train y test
- [ ] No hay valores NaN en los datos transformados

---

## üéØ PR√ìXIMOS PASOS

Una vez completada la Fase 2:

1. ‚úÖ Verifica que todos los archivos se guardaron correctamente
2. ‚úÖ Revisa el resumen final del notebook
3. ‚úÖ Av√≠same cuando est√©s listo para la Fase 3

**Fase 3**: Entrenamiento y Evaluaci√≥n de Modelos
- Necesitar√°s: scikit-learn, xgboost, lightgbm (opcional)
- Archivos de entrada: X_train_transformed.csv, y_train.csv, etc.

---

## üìù NOTAS IMPORTANTES

1. **Estratificaci√≥n**: El split de datos usa estratificaci√≥n para mantener la proporci√≥n de clases (importante para datasets desbalanceados)

2. **RobustScaler**: Se usa RobustScaler en lugar de StandardScaler porque es m√°s robusto a outliers (recomendado para este dataset)

3. **OneHotEncoder**: Se usa `drop='first'` para evitar multicolinealidad

4. **Data Leakage**: El split se hace ANTES de transformar para evitar data leakage

5. **Reproducibilidad**: Se usa `random_state=42` para garantizar resultados reproducibles

---

**Fecha de creaci√≥n**: Noviembre 2025
**Autor**: Alejandro Pineda Alvarez
**Proyecto**: Marketing Campaign Response Prediction

