{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 2: Ingenier√≠a de Caracter√≠sticas (Feature Engineering)\n",
    "# Marketing Campaign Response Prediction\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Este notebook implementa el pipeline completo de ingenier√≠a de caracter√≠sticas:\n",
    "\n",
    "1. **Limpieza de datos**: Manejo de nulos, outliers, inconsistencias\n",
    "2. **Creaci√≥n de features derivados**: Nuevas variables calculadas\n",
    "3. **Transformaci√≥n de variables**: Escalado y encoding\n",
    "4. **Split de datos**: Train/Test estratificado\n",
    "5. **Pipeline de preprocesamiento**: Listo para modelado\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Manipulaci√≥n de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Machine Learning - Scikit-learn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "\n",
    "# Configuraci√≥n\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print('‚úÖ Librer√≠as importadas correctamente')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cargar configuraci√≥n\n",
    "with open('../../config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print('Configuraci√≥n cargada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cargar dataset\n",
    "# Opci√≥n 1: Cargar desde Base_de_datos.csv (si no ejecutaste Fase 1)\n",
    "# data_path = f'../../{config[\"data_path\"]}'\n",
    "# df = pd.read_csv(data_path, sep=';')\n",
    "\n",
    "# Opci√≥n 2: Cargar dataset con features derivados de la Fase 1 (recomendado)\n",
    "try:\n",
    "    df = pd.read_csv('../../data_with_features.csv')\n",
    "    print('‚úÖ Dataset con features derivados cargado desde data_with_features.csv')\n",
    "except FileNotFoundError:\n",
    "    # Si no existe, cargar desde Base_de_datos.csv\n",
    "    data_path = f'../../{config[\"data_path\"]}'\n",
    "    df = pd.read_csv(data_path, sep=';')\n",
    "    print('‚úÖ Dataset cargado desde Base_de_datos.csv')\n",
    "\n",
    "print(f'\\nDimensiones: {df.shape[0]} filas √ó {df.shape[1]} columnas')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. LIMPIEZA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print('\\n' + '='*80)\n",
    "print('LIMPIEZA DE DATOS')\n",
    "print('='*80)\n",
    "print(f'\\nDimensiones iniciales: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Eliminar variables irrelevantes\n",
    "cols_to_drop = ['ID', 'Z_CostContact', 'Z_Revenue']\n",
    "cols_to_drop = [col for col in cols_to_drop if col in df.columns]\n",
    "\n",
    "if cols_to_drop:\n",
    "    df_clean = df.drop(columns=cols_to_drop)\n",
    "    print(f'\\n‚úÖ Variables eliminadas: {cols_to_drop}')\n",
    "else:\n",
    "    df_clean = df.copy()\n",
    "    print('\\n‚úÖ No hay variables irrelevantes para eliminar')\n",
    "\n",
    "print(f'Dimensiones despu√©s de limpieza: {df_clean.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convertir tipos de datos\n",
    "if 'Dt_Customer' in df_clean.columns:\n",
    "    df_clean['Dt_Customer'] = pd.to_datetime(df_clean['Dt_Customer'], format='%Y-%m-%d', errors='coerce')\n",
    "    print('‚úÖ Dt_Customer convertido a datetime')\n",
    "\n",
    "# Convertir variables categ√≥ricas\n",
    "if 'Education' in df_clean.columns:\n",
    "    df_clean['Education'] = df_clean['Education'].astype('category')\n",
    "if 'Marital_Status' in df_clean.columns:\n",
    "    df_clean['Marital_Status'] = df_clean['Marital_Status'].astype('category')\n",
    "\n",
    "print('‚úÖ Tipos de datos convertidos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Manejo de valores nulos\n",
    "if 'Income' in df_clean.columns:\n",
    "    nulos_income = df_clean['Income'].isnull().sum()\n",
    "    if nulos_income > 0:\n",
    "        print(f'\\n‚ö†Ô∏è Valores nulos en Income: {nulos_income} ({nulos_income/len(df_clean)*100:.2f}%)')\n",
    "        median_income = df_clean['Income'].median()\n",
    "        df_clean['Income'] = df_clean['Income'].fillna(median_income)\n",
    "        print(f'‚úÖ Imputados con mediana: {median_income:.2f}')\n",
    "    else:\n",
    "        print('\\n‚úÖ Income no tiene valores nulos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Unificar categor√≠as en Education\n",
    "if 'Education' in df_clean.columns:\n",
    "    education_mapping = {\n",
    "        '2n Cycle': 'Undergraduate',\n",
    "        'Basic': 'Basic',\n",
    "        'Graduation': 'Graduate',\n",
    "        'Master': 'Postgraduate',\n",
    "        'PhD': 'Postgraduate'\n",
    "    }\n",
    "    df_clean['Education'] = df_clean['Education'].map(education_mapping)\n",
    "    print('\\n‚úÖ Education unificado')\n",
    "    print(df_clean['Education'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Unificar categor√≠as en Marital_Status\n",
    "if 'Marital_Status' in df_clean.columns:\n",
    "    marital_mapping = {\n",
    "        'Single': 'Single',\n",
    "        'Together': 'Relationship',\n",
    "        'Married': 'Relationship',\n",
    "        'Divorced': 'Single',\n",
    "        'Widow': 'Single',\n",
    "        'Alone': 'Single',\n",
    "        'Absurd': 'Other',\n",
    "        'YOLO': 'Other'\n",
    "    }\n",
    "    df_clean['Marital_Status'] = df_clean['Marital_Status'].map(\n",
    "        lambda x: marital_mapping.get(x, 'Other')\n",
    "    )\n",
    "    print('\\n‚úÖ Marital_Status unificado')\n",
    "    print(df_clean['Marital_Status'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. CREACI√ìN DE FEATURES DERIVADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print('\\n' + '='*80)\n",
    "print('CREACI√ìN DE FEATURES DERIVADOS')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Features de gastos y compras\n",
    "gastos_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', \n",
    "               'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "gastos_cols = [col for col in gastos_cols if col in df_clean.columns]\n",
    "\n",
    "purchases_cols = ['NumDealsPurchases', 'NumWebPurchases', \n",
    "                 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "purchases_cols = [col for col in purchases_cols if col in df_clean.columns]\n",
    "\n",
    "campaigns_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', \n",
    "                 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "campaigns_cols = [col for col in campaigns_cols if col in df_clean.columns]\n",
    "\n",
    "# Crear features\n",
    "if 'TotalSpent' not in df_clean.columns and gastos_cols:\n",
    "    df_clean['TotalSpent'] = df_clean[gastos_cols].sum(axis=1)\n",
    "    print('‚úÖ TotalSpent creado')\n",
    "\n",
    "if 'TotalPurchases' not in df_clean.columns and purchases_cols:\n",
    "    df_clean['TotalPurchases'] = df_clean[purchases_cols].sum(axis=1)\n",
    "    print('‚úÖ TotalPurchases creado')\n",
    "\n",
    "if 'AvgPurchaseValue' not in df_clean.columns:\n",
    "    df_clean['AvgPurchaseValue'] = df_clean['TotalSpent'] / (df_clean['TotalPurchases'] + 1)\n",
    "    print('‚úÖ AvgPurchaseValue creado')\n",
    "\n",
    "if 'TotalCampaignsAccepted' not in df_clean.columns and campaigns_cols:\n",
    "    df_clean['TotalCampaignsAccepted'] = df_clean[campaigns_cols].sum(axis=1)\n",
    "    print('‚úÖ TotalCampaignsAccepted creado')\n",
    "\n",
    "if 'HasChildren' not in df_clean.columns:\n",
    "    df_clean['HasChildren'] = ((df_clean['Kidhome'] + df_clean['Teenhome']) > 0).astype(int)\n",
    "    print('‚úÖ HasChildren creado')\n",
    "\n",
    "if 'TotalChildren' not in df_clean.columns:\n",
    "    df_clean['TotalChildren'] = df_clean['Kidhome'] + df_clean['Teenhome']\n",
    "    print('‚úÖ TotalChildren creado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Features temporales\n",
    "if 'Age' not in df_clean.columns and 'Year_Birth' in df_clean.columns:\n",
    "    df_clean['Age'] = 2014 - df_clean['Year_Birth']\n",
    "    print('‚úÖ Age creado')\n",
    "\n",
    "if 'CustomerTenure' not in df_clean.columns and 'Dt_Customer' in df_clean.columns:\n",
    "    reference_date = df_clean['Dt_Customer'].max()\n",
    "    df_clean['CustomerTenure'] = (reference_date - df_clean['Dt_Customer']).dt.days\n",
    "    print('‚úÖ CustomerTenure creado')\n",
    "\n",
    "if 'WebEngagement' not in df_clean.columns:\n",
    "    df_clean['WebEngagement'] = df_clean['NumWebPurchases'] / (df_clean['NumWebVisitsMonth'] + 1)\n",
    "    print('‚úÖ WebEngagement creado')\n",
    "\n",
    "if 'IncomePerPerson' not in df_clean.columns:\n",
    "    df_clean['IncomePerPerson'] = df_clean['Income'] / (1 + df_clean['TotalChildren'])\n",
    "    print('‚úÖ IncomePerPerson creado')\n",
    "\n",
    "if 'SpendingRatio' not in df_clean.columns:\n",
    "    df_clean['SpendingRatio'] = df_clean['TotalSpent'] / (df_clean['Income'] + 1)\n",
    "    print('‚úÖ SpendingRatio creado')\n",
    "\n",
    "print(f'\\n‚úÖ Total de features derivados: 12')\n",
    "print(f'Dimensiones finales: {df_clean.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. PREPARACI√ìN PARA MODELADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Separar X e y\n",
    "target_col = 'Response'\n",
    "y = df_clean[target_col]\n",
    "X = df_clean.drop(columns=[target_col])\n",
    "\n",
    "# Eliminar columnas no necesarias\n",
    "cols_to_drop_model = ['Dt_Customer', 'Year_Birth']\n",
    "cols_to_drop_model = [col for col in cols_to_drop_model if col in X.columns]\n",
    "if cols_to_drop_model:\n",
    "    X = X.drop(columns=cols_to_drop_model)\n",
    "\n",
    "print(f'Dimensiones de X: {X.shape}')\n",
    "print(f'Dimensiones de y: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identificar tipos de variables\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64', 'int8']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f'Variables num√©ricas: {len(numeric_features)}')\n",
    "print(f'Variables categ√≥ricas: {len(categorical_features)}')\n",
    "print(f'\\nCateg√≥ricas: {categorical_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. CREACI√ìN DE PIPELINE DE PREPROCESAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Pipeline para variables num√©ricas\n",
    "use_robust_scaler = True\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler() if use_robust_scaler else StandardScaler())\n",
    "])\n",
    "\n",
    "print(f'‚úÖ Pipeline num√©rico creado')\n",
    "print(f'   Scaler: {\"RobustScaler\" if use_robust_scaler else \"StandardScaler\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Pipeline para variables categ√≥ricas\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "print(f'‚úÖ Pipeline categ√≥rico creado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Combinar pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "print(f'‚úÖ Preprocessor creado')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. SPLIT DE DATOS (TRAIN/TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Dividir datos (antes de transformar)\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=test_size, \n",
    "    random_state=random_state,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f'Test size: {test_size*100:.0f}%')\n",
    "print(f'\\nDimensiones:')\n",
    "print(f'  X_train: {X_train.shape}')\n",
    "print(f'  X_test:  {X_test.shape}')\n",
    "print(f'  y_train: {y_train.shape}')\n",
    "print(f'  y_test:  {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verificar distribuci√≥n de clases\n",
    "print(f'Distribuci√≥n de clases en Train:')\n",
    "print(y_train.value_counts())\n",
    "print(f'\\nDistribuci√≥n de clases en Test:')\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Transformar datos\n",
    "print('Transformando datos de entrenamiento...')\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "print('Transformando datos de prueba...')\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f'\\n‚úÖ Datos transformados')\n",
    "print(f'   X_train_transformed: {X_train_transformed.shape}')\n",
    "print(f'   X_test_transformed:  {X_test_transformed.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Obtener nombres de features despu√©s de transformaci√≥n\n",
    "feature_names = []\n",
    "feature_names.extend(numeric_features)\n",
    "\n",
    "if len(categorical_features) > 0:\n",
    "    cat_encoder = preprocessor.named_transformers_['cat']['onehot']\n",
    "    cat_feature_names = cat_encoder.get_feature_names_out(categorical_features)\n",
    "    feature_names.extend(cat_feature_names)\n",
    "\n",
    "print(f'Total de features: {len(feature_names)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. GUARDAR RESULTADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Guardar preprocessor\n",
    "preprocessor_path = '../../preprocessor.pkl'\n",
    "joblib.dump(preprocessor, preprocessor_path)\n",
    "print(f'‚úÖ Preprocessor guardado en: {preprocessor_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Guardar dataset procesado\n",
    "df_processed_path = '../../data_processed.csv'\n",
    "df_clean.to_csv(df_processed_path, index=False)\n",
    "print(f'‚úÖ Dataset procesado guardado en: {df_processed_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Guardar datos transformados (opcional)\n",
    "X_train_df = pd.DataFrame(X_train_transformed, columns=feature_names)\n",
    "X_test_df = pd.DataFrame(X_test_transformed, columns=feature_names)\n",
    "\n",
    "X_train_df.to_csv('../../X_train_transformed.csv', index=False)\n",
    "X_test_df.to_csv('../../X_test_transformed.csv', index=False)\n",
    "y_train.to_csv('../../y_train.csv', index=False)\n",
    "y_test.to_csv('../../y_test.csv', index=False)\n",
    "\n",
    "print(f'‚úÖ Datos transformados guardados')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. RESUMEN FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print('\\n' + '='*80)\n",
    "print('RESUMEN FINAL - FASE 2')\n",
    "print('='*80)\n",
    "\n",
    "print('\\n‚úÖ FASE 2 COMPLETADA EXITOSAMENTE')\n",
    "print(f'\\nüìä Resumen:')\n",
    "print(f'  1. Dataset procesado: {df_clean.shape[0]} registros, {df_clean.shape[1]} variables')\n",
    "print(f'  2. Features derivados: 12')\n",
    "print(f'  3. Variables num√©ricas: {len(numeric_features)}')\n",
    "print(f'  4. Variables categ√≥ricas: {len(categorical_features)}')\n",
    "print(f'  5. Features despu√©s de transformaci√≥n: {len(feature_names)}')\n",
    "print(f'  6. Train set: {X_train_transformed.shape}')\n",
    "print(f'  7. Test set: {X_test_transformed.shape}')\n",
    "\n",
    "print('\\nüìÅ Archivos generados:')\n",
    "print('  - preprocessor.pkl')\n",
    "print('  - data_processed.csv')\n",
    "print('  - X_train_transformed.csv')\n",
    "print('  - X_test_transformed.csv')\n",
    "print('  - y_train.csv')\n",
    "print('  - y_test.csv')\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('üéâ FASE 2 COMPLETADA - LISTO PARA FASE 3')\n",
    "print('='*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}