"""
ft_engineering.py
Ingenier√≠a de Caracter√≠sticas y Preparaci√≥n de Datos para Modelado.

Este m√≥dulo contiene todas las funciones necesarias para:
- Limpieza de datos (nulos, outliers, inconsistencias)
- Creaci√≥n de features derivados
- Transformaci√≥n de variables (escalado, encoding)
- Split de datos (train/test estratificado)

Autor: Alejandro Pineda Alvarez
Proyecto: Marketing Campaign Response Prediction
"""

import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
import json
import warnings
warnings.filterwarnings('ignore')


# ============================================================================
# 1. LIMPIEZA DE DATOS
# ============================================================================

def clean_data(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:
    """
    Limpia el dataset: maneja nulos, outliers e inconsistencias.
    
    Args:
        df: DataFrame de entrada
        verbose: Si True, imprime informaci√≥n del proceso
        
    Returns:
        DataFrame limpio
    """
    df_clean = df.copy()
    
    if verbose:
        print("\n" + "="*80)
        print("üßπ LIMPIEZA DE DATOS")
        print("="*80)
        print(f"Dimensiones iniciales: {df_clean.shape}")
    
    # 1. Eliminar variables irrelevantes
    cols_to_drop = ['ID', 'Z_CostContact', 'Z_Revenue']
    cols_to_drop = [col for col in cols_to_drop if col in df_clean.columns]
    if cols_to_drop:
        df_clean = df_clean.drop(columns=cols_to_drop)
        if verbose:
            print(f"\n‚úÖ Variables eliminadas: {cols_to_drop}")
    
    # 2. Convertir tipos de datos
    if 'Dt_Customer' in df_clean.columns:
        df_clean['Dt_Customer'] = pd.to_datetime(df_clean['Dt_Customer'], format='%Y-%m-%d', errors='coerce')
    
    # 3. Manejo de valores nulos en Income
    if 'Income' in df_clean.columns:
        nulos_income = df_clean['Income'].isnull().sum()
        if nulos_income > 0:
            if verbose:
                print(f"\n‚ö†Ô∏è Valores nulos en Income: {nulos_income} ({nulos_income/len(df_clean)*100:.2f}%)")
            
            # Imputar con mediana (robusto a outliers)
            median_income = df_clean['Income'].median()
            df_clean['Income'] = df_clean['Income'].fillna(median_income)
            
            if verbose:
                print(f"‚úÖ Imputados con mediana: {median_income:.2f}")
    
    # 4. Manejo de outliers extremos en Income (opcional: usar IQR)
    if 'Income' in df_clean.columns:
        Q1 = df_clean['Income'].quantile(0.25)
        Q3 = df_clean['Income'].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 3 * IQR  # 3 IQR para outliers extremos
        upper_bound = Q3 + 3 * IQR
        
        outliers_count = ((df_clean['Income'] < lower_bound) | (df_clean['Income'] > upper_bound)).sum()
        
        if outliers_count > 0 and verbose:
            print(f"\n‚ö†Ô∏è Outliers extremos en Income: {outliers_count}")
            print(f"   Rango normal: [{lower_bound:.0f}, {upper_bound:.0f}]")
            # Nota: No eliminamos outliers, solo los reportamos
            # El RobustScaler manejar√° esto en la transformaci√≥n
    
    # 5. Unificar categor√≠as en Education
    if 'Education' in df_clean.columns:
        # Mapeo de categor√≠as similares
        education_mapping = {
            '2n Cycle': 'Undergraduate',
            'Basic': 'Basic',
            'Graduation': 'Graduate',
            'Master': 'Postgraduate',
            'PhD': 'Postgraduate'
        }
        df_clean['Education'] = df_clean['Education'].map(education_mapping)
        
        if verbose:
            print(f"\n‚úÖ Education unificado: {df_clean['Education'].value_counts().to_dict()}")
    
    # 6. Unificar categor√≠as en Marital_Status
    if 'Marital_Status' in df_clean.columns:
        # Agrupar categor√≠as poco frecuentes
        marital_mapping = {
            'Single': 'Single',
            'Together': 'Relationship',
            'Married': 'Relationship',
            'Divorced': 'Single',
            'Widow': 'Single',
            'Alone': 'Single',
            'Absurd': 'Other',
            'YOLO': 'Other'
        }
        df_clean['Marital_Status'] = df_clean['Marital_Status'].map(
            lambda x: marital_mapping.get(x, 'Other')
        )
        
        if verbose:
            print(f"‚úÖ Marital_Status unificado: {df_clean['Marital_Status'].value_counts().to_dict()}")
    
    if verbose:
        print(f"\nDimensiones finales: {df_clean.shape}")
        print("="*80)
    
    return df_clean


# ============================================================================
# 2. CREACI√ìN DE FEATURES DERIVADOS
# ============================================================================

def create_derived_features(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:
    """
    Crea features derivados a partir de variables existentes.
    
    Args:
        df: DataFrame de entrada
        verbose: Si True, imprime informaci√≥n del proceso
        
    Returns:
        DataFrame con features derivados
    """
    df_features = df.copy()
    
    if verbose:
        print("\n" + "="*80)
        print("üîß CREACI√ìN DE FEATURES DERIVADOS")
        print("="*80)
    
    # 1. Total Spent (suma de todos los gastos)
    gastos_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 
                   'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']
    gastos_cols = [col for col in gastos_cols if col in df_features.columns]
    
    if gastos_cols:
        df_features['TotalSpent'] = df_features[gastos_cols].sum(axis=1)
        if verbose:
            print(f"‚úÖ TotalSpent creado (suma de {len(gastos_cols)} variables de gasto)")
    
    # 2. Total Purchases (suma de todas las compras)
    purchases_cols = ['NumDealsPurchases', 'NumWebPurchases', 
                     'NumCatalogPurchases', 'NumStorePurchases']
    purchases_cols = [col for col in purchases_cols if col in df_features.columns]
    
    if purchases_cols:
        df_features['TotalPurchases'] = df_features[purchases_cols].sum(axis=1)
        if verbose:
            print(f"‚úÖ TotalPurchases creado (suma de {len(purchases_cols)} canales)")
    
    # 3. Average Purchase Value
    if 'TotalSpent' in df_features.columns and 'TotalPurchases' in df_features.columns:
        df_features['AvgPurchaseValue'] = df_features['TotalSpent'] / (df_features['TotalPurchases'] + 1)
        if verbose:
            print("‚úÖ AvgPurchaseValue creado (TotalSpent / TotalPurchases)")
    
    # 4. Total Accepted Campaigns
    campaigns_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 
                     'AcceptedCmp4', 'AcceptedCmp5']
    campaigns_cols = [col for col in campaigns_cols if col in df_features.columns]
    
    if campaigns_cols:
        df_features['TotalCampaignsAccepted'] = df_features[campaigns_cols].sum(axis=1)
        if verbose:
            print(f"‚úÖ TotalCampaignsAccepted creado (suma de {len(campaigns_cols)} campa√±as)")
    
    # 5. Has Children
    if 'Kidhome' in df_features.columns and 'Teenhome' in df_features.columns:
        df_features['HasChildren'] = ((df_features['Kidhome'] + df_features['Teenhome']) > 0).astype(int)
        if verbose:
            print("‚úÖ HasChildren creado (indicador binario)")
    
    # 6. Total Children
    if 'Kidhome' in df_features.columns and 'Teenhome' in df_features.columns:
        df_features['TotalChildren'] = df_features['Kidhome'] + df_features['Teenhome']
        if verbose:
            print("‚úÖ TotalChildren creado (Kidhome + Teenhome)")
    
    # 7. Age (desde Year_Birth)
    if 'Year_Birth' in df_features.columns:
        current_year = 2014  # A√±o de referencia del dataset
        df_features['Age'] = current_year - df_features['Year_Birth']
        if verbose:
            print(f"‚úÖ Age creado (edad calculada desde Year_Birth)")
    
    # 8. Customer Tenure (d√≠as desde inscripci√≥n)
    if 'Dt_Customer' in df_features.columns:
        reference_date = df_features['Dt_Customer'].max()
        df_features['CustomerTenure'] = (reference_date - df_features['Dt_Customer']).dt.days
        if verbose:
            print("‚úÖ CustomerTenure creado (d√≠as desde inscripci√≥n)")
    
    # 9. Web Engagement (tasa de conversi√≥n web)
    if 'NumWebPurchases' in df_features.columns and 'NumWebVisitsMonth' in df_features.columns:
        df_features['WebEngagement'] = df_features['NumWebPurchases'] / (df_features['NumWebVisitsMonth'] + 1)
        if verbose:
            print("‚úÖ WebEngagement creado (NumWebPurchases / NumWebVisitsMonth)")
    
    # 10. Income per Person
    if 'Income' in df_features.columns and 'TotalChildren' in df_features.columns:
        df_features['IncomePerPerson'] = df_features['Income'] / (1 + df_features['TotalChildren'])
        if verbose:
            print("‚úÖ IncomePerPerson creado (Income / (1 + TotalChildren))")
    
    # 11. Spending Ratio (proporci√≥n de ingreso gastado)
    if 'TotalSpent' in df_features.columns and 'Income' in df_features.columns:
        df_features['SpendingRatio'] = df_features['TotalSpent'] / (df_features['Income'] + 1)
        if verbose:
            print("‚úÖ SpendingRatio creado (TotalSpent / Income)")
    
    # 12. Days Since Last Purchase (inverso de Recency para mejor interpretaci√≥n)
    if 'Recency' in df_features.columns:
        df_features['DaysSinceLastPurchase'] = df_features['Recency']
        if verbose:
            print("‚úÖ DaysSinceLastPurchase creado (alias de Recency)")
    
    if verbose:
        print(f"\nTotal de features derivados creados: 12")
        print("="*80)
    
    return df_features


# ============================================================================
# 3. PREPARACI√ìN DE VARIABLES PARA MODELADO
# ============================================================================

def prepare_features_for_modeling(df: pd.DataFrame, target_col: str = 'Response', 
                                  verbose: bool = True) -> tuple:
    """
    Prepara las features para modelado: separa X e y, identifica tipos de variables.
    
    Args:
        df: DataFrame con features
        target_col: Nombre de la columna objetivo
        verbose: Si True, imprime informaci√≥n
        
    Returns:
        tuple: (X, y, numeric_features, categorical_features)
    """
    df_model = df.copy()
    
    if verbose:
        print("\n" + "="*80)
        print("üìä PREPARACI√ìN DE FEATURES PARA MODELADO")
        print("="*80)
    
    # Separar X e y
    if target_col not in df_model.columns:
        raise ValueError(f"Columna objetivo '{target_col}' no encontrada en el dataset")
    
    y = df_model[target_col]
    X = df_model.drop(columns=[target_col])
    
    # Eliminar columnas que no se usar√°n en el modelo
    cols_to_drop = ['Dt_Customer', 'Year_Birth']  # Ya creamos features derivados
    cols_to_drop = [col for col in cols_to_drop if col in X.columns]
    if cols_to_drop:
        X = X.drop(columns=cols_to_drop)
    
    # Identificar tipos de variables
    numeric_features = X.select_dtypes(include=['int64', 'float64', 'int8']).columns.tolist()
    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()
    
    if verbose:
        print(f"\nDimensiones de X: {X.shape}")
        print(f"Dimensiones de y: {y.shape}")
        print(f"\nVariables num√©ricas ({len(numeric_features)}): {numeric_features[:5]}...")
        print(f"Variables categ√≥ricas ({len(categorical_features)}): {categorical_features}")
        print(f"\nBalance de clases en y:")
        print(y.value_counts())
        print(f"Proporci√≥n: {y.value_counts(normalize=True).to_dict()}")
        print("="*80)
    
    return X, y, numeric_features, categorical_features


# ============================================================================
# 4. CREACI√ìN DE PIPELINES DE TRANSFORMACI√ìN
# ============================================================================

def create_preprocessing_pipeline(numeric_features: list, categorical_features: list,
                                 use_robust_scaler: bool = True) -> ColumnTransformer:
    """
    Crea un pipeline de preprocesamiento con transformaciones para variables num√©ricas y categ√≥ricas.
    
    Args:
        numeric_features: Lista de nombres de features num√©ricas
        categorical_features: Lista de nombres de features categ√≥ricas
        use_robust_scaler: Si True, usa RobustScaler (mejor para outliers), si no StandardScaler
        
    Returns:
        ColumnTransformer configurado
    """
    
    # Pipeline para variables num√©ricas
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),  # Imputar nulos con mediana
        ('scaler', RobustScaler() if use_robust_scaler else StandardScaler())  # Escalado
    ])
    
    # Pipeline para variables categ√≥ricas
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),  # Imputar nulos
        ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))  # One-hot encoding
    ])
    
    # Combinar pipelines
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features)
        ],
        remainder='drop'  # Eliminar columnas no especificadas
    )
    
    return preprocessor


# ============================================================================
# 5. SPLIT DE DATOS
# ============================================================================

def split_data(X: pd.DataFrame, y: pd.Series, test_size: float = 0.2, 
               random_state: int = 42, stratify: bool = True, 
               verbose: bool = True) -> tuple:
    """
    Divide los datos en conjuntos de entrenamiento y prueba.
    
    Args:
        X: Features
        y: Variable objetivo
        test_size: Proporci√≥n del conjunto de prueba (default: 0.2)
        random_state: Semilla aleatoria para reproducibilidad
        stratify: Si True, mantiene la proporci√≥n de clases (recomendado para datasets desbalanceados)
        verbose: Si True, imprime informaci√≥n
        
    Returns:
        tuple: (X_train, X_test, y_train, y_test)
    """
    
    if verbose:
        print("\n" + "="*80)
        print("‚úÇÔ∏è SPLIT DE DATOS")
        print("="*80)
    
    stratify_param = y if stratify else None
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, 
        test_size=test_size, 
        random_state=random_state,
        stratify=stratify_param
    )
    
    if verbose:
        print(f"\nTest size: {test_size*100:.0f}%")
        print(f"Random state: {random_state}")
        print(f"Estratificaci√≥n: {'S√≠' if stratify else 'No'}")
        print(f"\nDimensiones:")
        print(f"  X_train: {X_train.shape}")
        print(f"  X_test:  {X_test.shape}")
        print(f"  y_train: {y_train.shape}")
        print(f"  y_test:  {y_test.shape}")
        
        print(f"\nDistribuci√≥n de clases:")
        print(f"  Train: {y_train.value_counts().to_dict()}")
        print(f"  Test:  {y_test.value_counts().to_dict()}")
        print(f"\n  Proporci√≥n train: {y_train.value_counts(normalize=True).to_dict()}")
        print(f"  Proporci√≥n test:  {y_test.value_counts(normalize=True).to_dict()}")
        print("="*80)
    
    return X_train, X_test, y_train, y_test


# ============================================================================
# 6. FUNCI√ìN PRINCIPAL - PIPELINE COMPLETO
# ============================================================================

def run_feature_engineering_pipeline(data_path: str = None, df: pd.DataFrame = None,
                                    test_size: float = 0.2, random_state: int = 42,
                                    use_robust_scaler: bool = True,
                                    save_preprocessor: bool = True,
                                    verbose: bool = True) -> dict:
    """
    Ejecuta el pipeline completo de ingenier√≠a de caracter√≠sticas.
    
    Args:
        data_path: Ruta al archivo CSV (si df es None)
        df: DataFrame de entrada (si data_path es None)
        test_size: Proporci√≥n del conjunto de prueba
        random_state: Semilla aleatoria
        use_robust_scaler: Si True, usa RobustScaler
        save_preprocessor: Si True, guarda el preprocessor
        verbose: Si True, imprime informaci√≥n detallada
        
    Returns:
        dict con todos los componentes:
            - X_train, X_test, y_train, y_test: Datos divididos
            - preprocessor: Pipeline de transformaci√≥n
            - numeric_features, categorical_features: Listas de features
            - df_processed: DataFrame procesado completo
    """
    
    if verbose:
        print("\n" + "="*80)
        print("üöÄ PIPELINE DE INGENIER√çA DE CARACTER√çSTICAS")
        print("="*80)
    
    # 1. Cargar datos
    if df is None:
        if data_path is None:
            raise ValueError("Debe proporcionar data_path o df")
        df = pd.read_csv(data_path, sep=';')
        if verbose:
            print(f"\n‚úÖ Datos cargados desde: {data_path}")
            print(f"   Dimensiones: {df.shape}")
    
    # 2. Limpieza de datos
    df_clean = clean_data(df, verbose=verbose)
    
    # 3. Creaci√≥n de features derivados
    df_features = create_derived_features(df_clean, verbose=verbose)
    
    # 4. Preparaci√≥n para modelado
    X, y, numeric_features, categorical_features = prepare_features_for_modeling(
        df_features, target_col='Response', verbose=verbose
    )
    
    # 5. Split de datos
    X_train, X_test, y_train, y_test = split_data(
        X, y, test_size=test_size, random_state=random_state, 
        stratify=True, verbose=verbose
    )
    
    # 6. Crear preprocessor
    if verbose:
        print("\n" + "="*80)
        print("üîß CREACI√ìN DE PREPROCESSOR")
        print("="*80)
        print(f"\nScaler: {'RobustScaler' if use_robust_scaler else 'StandardScaler'}")
        print(f"Encoding: OneHotEncoder (drop='first')")
    
    preprocessor = create_preprocessing_pipeline(
        numeric_features, categorical_features, use_robust_scaler
    )
    
    # 7. Guardar preprocessor (opcional)
    if save_preprocessor:
        import joblib
        joblib.dump(preprocessor, 'preprocessor.pkl')
        if verbose:
            print(f"\n‚úÖ Preprocessor guardado en: preprocessor.pkl")
    
    if verbose:
        print("\n" + "="*80)
        print("‚úÖ PIPELINE COMPLETADO EXITOSAMENTE")
        print("="*80)
    
    # Retornar todos los componentes
    return {
        'X_train': X_train,
        'X_test': X_test,
        'y_train': y_train,
        'y_test': y_test,
        'preprocessor': preprocessor,
        'numeric_features': numeric_features,
        'categorical_features': categorical_features,
        'df_processed': df_features,
        'feature_names': X.columns.tolist()
    }


# ============================================================================
# 7. FUNCI√ìN DE UTILIDAD - OBTENER NOMBRES DE FEATURES TRANSFORMADOS
# ============================================================================

def get_feature_names_after_preprocessing(preprocessor, numeric_features, categorical_features):
    """
    Obtiene los nombres de las features despu√©s de aplicar el preprocessor.
    
    Args:
        preprocessor: ColumnTransformer fitted
        numeric_features: Lista de features num√©ricas originales
        categorical_features: Lista de features categ√≥ricas originales
        
    Returns:
        Lista con nombres de todas las features transformadas
    """
    feature_names = []
    
    # Features num√©ricas (mantienen su nombre)
    feature_names.extend(numeric_features)
    
    # Features categ√≥ricas (one-hot encoded)
    if len(categorical_features) > 0:
        cat_encoder = preprocessor.named_transformers_['cat']['onehot']
        cat_feature_names = cat_encoder.get_feature_names_out(categorical_features)
        feature_names.extend(cat_feature_names)
    
    return feature_names


# ============================================================================
# EJEMPLO DE USO
# ============================================================================

if __name__ == "__main__":
    print("\n" + "="*80)
    print("EJEMPLO DE USO - FEATURE ENGINEERING PIPELINE")
    print("="*80)
    
    # Cargar configuraci√≥n
    with open('../../config.json', 'r') as f:
        config = json.load(f)
    
    data_path = f'../../{config["data_path"]}'
    
    # Ejecutar pipeline completo
    results = run_feature_engineering_pipeline(
        data_path=data_path,
        test_size=0.2,
        random_state=42,
        use_robust_scaler=True,
        save_preprocessor=True,
        verbose=True
    )
    
    print("\n" + "="*80)
    print("üì¶ COMPONENTES DISPONIBLES:")
    print("="*80)
    print(f"  - X_train: {results['X_train'].shape}")
    print(f"  - X_test: {results['X_test'].shape}")
    print(f"  - y_train: {results['y_train'].shape}")
    print(f"  - y_test: {results['y_test'].shape}")
    print(f"  - preprocessor: {type(results['preprocessor'])}")
    print(f"  - numeric_features: {len(results['numeric_features'])} features")
    print(f"  - categorical_features: {len(results['categorical_features'])} features")
    print(f"  - df_processed: {results['df_processed'].shape}")
    print(f"  - feature_names: {len(results['feature_names'])} features")
    
    print("\n‚úÖ Pipeline ejecutado exitosamente!")
    print("="*80)
