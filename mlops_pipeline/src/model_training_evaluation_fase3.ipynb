{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 3: Entrenamiento y Evaluaci√≥n de Modelos\n",
    "# Marketing Campaign Response Prediction\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Este notebook implementa el entrenamiento y evaluaci√≥n de m√∫ltiples modelos de Machine Learning:\n",
    "\n",
    "1. **Carga de datos**: Datos transformados de la Fase 2\n",
    "2. **Entrenamiento de modelos**: 7 algoritmos diferentes\n",
    "3. **Evaluaci√≥n**: M√©tricas completas (accuracy, precision, recall, F1, ROC-AUC)\n",
    "4. **Validaci√≥n cruzada**: 5-fold estratificada\n",
    "5. **Comparaci√≥n de modelos**: Tabla comparativa y visualizaciones\n",
    "6. **Selecci√≥n del mejor modelo**: Basado en m√©tricas y consistencia\n",
    "7. **Guardado del modelo**: Modelo final listo para despliegue\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importaci√≥n de Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Manipulaci√≥n de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Visualizaci√≥n\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning - Scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, roc_curve, auc, confusion_matrix, \n",
    "    classification_report, average_precision_score\n",
    ")\n",
    "\n",
    "# XGBoost y LightGBM\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "\n",
    "# Configuraci√≥n\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "print('‚úÖ Librer√≠as importadas correctamente')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga de Datos Transformados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print('\\n' + '='*80)\n",
    "print('CARGA DE DATOS')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cargar datos transformados de la Fase 2\n",
    "# Opci√≥n 1: Cargar desde archivos CSV (recomendado)\n",
    "try:\n",
    "    X_train = pd.read_csv('../../X_train_transformed.csv')\n",
    "    X_test = pd.read_csv('../../X_test_transformed.csv')\n",
    "    y_train = pd.read_csv('../../y_train.csv', squeeze=True)\n",
    "    y_test = pd.read_csv('../../y_test.csv', squeeze=True)\n",
    "    \n",
    "    # Convertir a numpy arrays\n",
    "    X_train = X_train.values\n",
    "    X_test = X_test.values\n",
    "    y_train = y_train.values\n",
    "    y_test = y_test.values\n",
    "    \n",
    "    print('‚úÖ Datos cargados desde archivos CSV')\n",
    "    print(f'   X_train: {X_train.shape}')\n",
    "    print(f'   X_test: {X_test.shape}')\n",
    "    print(f'   y_train: {y_train.shape}')\n",
    "    print(f'   y_test: {y_test.shape}')\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f'‚ùå Error: No se encontraron los archivos de la Fase 2')\n",
    "    print(f'   Aseg√∫rate de haber ejecutado la Fase 2 primero')\n",
    "    print(f'   Archivos necesarios:')\n",
    "    print(f'     - X_train_transformed.csv')\n",
    "    print(f'     - X_test_transformed.csv')\n",
    "    print(f'     - y_train.csv')\n",
    "    print(f'     - y_test.csv')\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verificar distribuci√≥n de clases\n",
    "print('\\nDistribuci√≥n de clases:')\n",
    "print(f'Train - No acepta (0): {(y_train == 0).sum()} ({(y_train == 0).mean()*100:.2f}%)')\n",
    "print(f'Train - Acepta (1):    {(y_train == 1).sum()} ({(y_train == 1).mean()*100:.2f}%)')\n",
    "print(f'Test - No acepta (0):  {(y_test == 0).sum()} ({(y_test == 0).mean()*100:.2f}%)')\n",
    "print(f'Test - Acepta (1):     {(y_test == 1).sum()} ({(y_test == 1).mean()*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cargar preprocessor (opcional, para obtener nombres de features)\n",
    "try:\n",
    "    preprocessor = joblib.load('../../preprocessor.pkl')\n",
    "    print('\\n‚úÖ Preprocessor cargado')\n",
    "except FileNotFoundError:\n",
    "    print('\\n‚ö†Ô∏è Preprocessor no encontrado (no cr√≠tico)')\n",
    "    preprocessor = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. DEFINICI√ìN DE MODELOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print('\\n' + '='*80)\n",
    "print('DEFINICI√ìN DE MODELOS')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calcular peso para balanceo de clases\n",
    "n_classes_0 = (y_train == 0).sum()\n",
    "n_classes_1 = (y_train == 1).sum()\n",
    "scale_pos_weight = n_classes_0 / n_classes_1\n",
    "\n",
    "print(f'\\nBalance de clases:')\n",
    "print(f'  Clase 0 (No acepta): {n_classes_0}')\n",
    "print(f'  Clase 1 (Acepta):    {n_classes_1}')\n",
    "print(f'  Ratio: {scale_pos_weight:.2f}:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Definir modelos\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000, \n",
    "        random_state=42, \n",
    "        class_weight='balanced'\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        random_state=42, \n",
    "        class_weight='balanced', \n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=100, \n",
    "        random_state=42\n",
    "    ),\n",
    "    'Extra Trees': ExtraTreesClassifier(\n",
    "        n_estimators=100, \n",
    "        random_state=42, \n",
    "        class_weight='balanced', \n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'SVM': SVC(\n",
    "        kernel='rbf', \n",
    "        probability=True, \n",
    "        random_state=42, \n",
    "        class_weight='balanced'\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=100, \n",
    "        random_state=42, \n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        use_label_encoder=False, \n",
    "        eval_metric='logloss'\n",
    "    ),\n",
    "    'LightGBM': LGBMClassifier(\n",
    "        n_estimators=100, \n",
    "        random_state=42, \n",
    "        class_weight='balanced', \n",
    "        verbose=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f'\\n‚úÖ {len(models)} modelos definidos:')\n",
    "for name in models.keys():\n",
    "    print(f'  - {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. FUNCI√ìN DE ENTRENAMIENTO Y EVALUACI√ìN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def build_model(model, X_train, y_train, X_test, y_test, model_name=\"Model\", cv_folds=5):\n",
    "    \"\"\"\n",
    "    Entrena y eval√∫a un modelo de clasificaci√≥n.\n",
    "    \"\"\"\n",
    "    print(f'\\n{\"=\"*80}')\n",
    "    print(f'ENTRENANDO: {model_name}')\n",
    "    print(f'{\"=\"*80}')\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    start_time = datetime.now()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    print(f'‚úÖ Modelo entrenado en {training_time:.2f} segundos')\n",
    "    \n",
    "    # Predicciones\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Probabilidades\n",
    "    try:\n",
    "        y_train_proba = model.predict_proba(X_train)[:, 1]\n",
    "        y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    except AttributeError:\n",
    "        y_train_proba = None\n",
    "        y_test_proba = None\n",
    "    \n",
    "    # M√©tricas en train\n",
    "    train_metrics = {\n",
    "        'accuracy': accuracy_score(y_train, y_train_pred),\n",
    "        'precision': precision_score(y_train, y_train_pred, zero_division=0),\n",
    "        'recall': recall_score(y_train, y_train_pred, zero_division=0),\n",
    "        'f1': f1_score(y_train, y_train_pred, zero_division=0)\n",
    "    }\n",
    "    \n",
    "    if y_train_proba is not None:\n",
    "        train_metrics['roc_auc'] = roc_auc_score(y_train, y_train_proba)\n",
    "    \n",
    "    # M√©tricas en test\n",
    "    test_metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_test_pred),\n",
    "        'precision': precision_score(y_test, y_test_pred, zero_division=0),\n",
    "        'recall': recall_score(y_test, y_test_pred, zero_division=0),\n",
    "        'f1': f1_score(y_test, y_test_pred, zero_division=0)\n",
    "    }\n",
    "    \n",
    "    if y_test_proba is not None:\n",
    "        test_metrics['roc_auc'] = roc_auc_score(y_test, y_test_proba)\n",
    "    \n",
    "    # Validaci√≥n cruzada\n",
    "    cv_scores = {}\n",
    "    if cv_folds > 0:\n",
    "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "        scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "        cv_results = cross_validate(model, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "        \n",
    "        for metric in scoring:\n",
    "            cv_scores[f'{metric}_mean'] = cv_results[f'test_{metric}'].mean()\n",
    "            cv_scores[f'{metric}_std'] = cv_results[f'test_{metric}'].std()\n",
    "    \n",
    "    # Overfitting check\n",
    "    overfitting = {\n",
    "        'accuracy_diff': train_metrics['accuracy'] - test_metrics['accuracy'],\n",
    "        'f1_diff': train_metrics['f1'] - test_metrics['f1']\n",
    "    }\n",
    "    \n",
    "    # Imprimir m√©tricas\n",
    "    print(f'\\nüìä M√©tricas en Test:')\n",
    "    print(f'   Accuracy:  {test_metrics[\"accuracy\"]:.4f}')\n",
    "    print(f'   Precision: {test_metrics[\"precision\"]:.4f}')\n",
    "    print(f'   Recall:    {test_metrics[\"recall\"]:.4f}')\n",
    "    print(f'   F1-Score:  {test_metrics[\"f1\"]:.4f}')\n",
    "    if 'roc_auc' in test_metrics:\n",
    "        print(f'   ROC-AUC:   {test_metrics[\"roc_auc\"]:.4f}')\n",
    "    \n",
    "    if cv_folds > 0:\n",
    "        print(f'\\nüìä Validaci√≥n Cruzada ({cv_folds}-fold):')\n",
    "        print(f'   F1-Score: {cv_scores[\"f1_mean\"]:.4f} ¬± {cv_scores[\"f1_std\"]:.4f}')\n",
    "        print(f'   ROC-AUC:  {cv_scores[\"roc_auc_mean\"]:.4f} ¬± {cv_scores[\"roc_auc_std\"]:.4f}')\n",
    "    \n",
    "    print(f'\\n‚ö†Ô∏è Overfitting Check:')\n",
    "    print(f'   Accuracy diff: {overfitting[\"accuracy_diff\"]:.4f}')\n",
    "    print(f'   F1 diff:       {overfitting[\"f1_diff\"]:.4f}')\n",
    "    \n",
    "    if overfitting['f1_diff'] > 0.1:\n",
    "        print(f'   ‚ö†Ô∏è Posible overfitting detectado')\n",
    "    else:\n",
    "        print(f'   ‚úÖ Modelo generaliza bien')\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'model_name': model_name,\n",
    "        'y_train_pred': y_train_pred,\n",
    "        'y_test_pred': y_test_pred,\n",
    "        'y_train_proba': y_train_proba,\n",
    "        'y_test_proba': y_test_proba,\n",
    "        'train_metrics': train_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'cv_scores': cv_scores,\n",
    "        'overfitting': overfitting,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "\n",
    "print('‚úÖ Funci√≥n build_model definida')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. ENTRENAMIENTO DE MODELOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print('\\n' + '='*80)\n",
    "print('ENTRENAMIENTO DE M√öLTIPLES MODELOS')\n",
    "print('='*80)\n",
    "print(f'Train set: {X_train.shape}')\n",
    "print(f'Test set:  {X_test.shape}')\n",
    "print(f'CV folds:  5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Entrenar todos los modelos\n",
    "results = {}\n",
    "cv_folds = 5\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    try:\n",
    "        result = build_model(\n",
    "            model, X_train, y_train, X_test, y_test,\n",
    "            model_name=model_name, cv_folds=cv_folds\n",
    "        )\n",
    "        results[model_name] = result\n",
    "    except Exception as e:\n",
    "        print(f'\\n‚ùå Error entrenando {model_name}: {str(e)}')\n",
    "        continue\n",
    "\n",
    "print(f'\\n{\"=\"*80}')\n",
    "print(f'‚úÖ Entrenamiento completado: {len(results)} modelos')\n",
    "print(f'{\"=\"*80}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. COMPARACI√ìN DE MODELOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print('\\n' + '='*80)\n",
    "print('COMPARACI√ìN DE MODELOS')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Crear DataFrame comparativo\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    test_metrics = result['test_metrics']\n",
    "    cv_scores = result.get('cv_scores', {})\n",
    "    \n",
    "    row = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': test_metrics['accuracy'],\n",
    "        'Precision': test_metrics['precision'],\n",
    "        'Recall': test_metrics['recall'],\n",
    "        'F1-Score': test_metrics['f1'],\n",
    "        'ROC-AUC': test_metrics.get('roc_auc', np.nan),\n",
    "        'CV_F1_mean': cv_scores.get('f1_mean', np.nan),\n",
    "        'CV_F1_std': cv_scores.get('f1_std', np.nan),\n",
    "        'CV_ROC-AUC_mean': cv_scores.get('roc_auc_mean', np.nan),\n",
    "        'Overfitting_F1': result['overfitting']['f1_diff'],\n",
    "        'Training_Time': result['training_time']\n",
    "    }\n",
    "    comparison_data.append(row)\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "df_comparison = df_comparison.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print('\\nüìä Tabla Comparativa (ordenada por F1-Score):\\n')\n",
    "print(df_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identificar mejor modelo\n",
    "best_model_name = df_comparison.iloc[0]['Model']\n",
    "best_result = results[best_model_name]\n",
    "\n",
    "print(f'\\nüèÜ Mejor Modelo: {best_model_name}')\n",
    "print(f'   F1-Score: {df_comparison.iloc[0][\"F1-Score\"]:.4f}')\n",
    "print(f'   ROC-AUC:  {df_comparison.iloc[0][\"ROC-AUC\"]:.4f}')\n",
    "print(f'   Accuracy: {df_comparison.iloc[0][\"Accuracy\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. VISUALIZACIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Gr√°fico 1: Comparaci√≥n de m√©tricas principales\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "df_plot = df_comparison[['Model'] + metrics_to_plot].set_index('Model')\n",
    "\n",
    "df_plot.plot(kind='bar', ax=ax, rot=45)\n",
    "ax.set_title('Comparaci√≥n de M√©tricas Principales', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Score')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../model_comparison_metrics.png', dpi=300, bbox_inches='tight')\n",
    "print('‚úÖ Gr√°fico guardado: model_comparison_metrics.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Gr√°fico 2: Comparaci√≥n de ROC-AUC\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "df_comparison_sorted = df_comparison.sort_values('ROC-AUC', ascending=True)\n",
    "ax.barh(df_comparison_sorted['Model'], df_comparison_sorted['ROC-AUC'], color='skyblue')\n",
    "ax.set_title('Comparaci√≥n de ROC-AUC', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('ROC-AUC Score')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "ax.set_xlim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../model_comparison_roc_auc.png', dpi=300, bbox_inches='tight')\n",
    "print('‚úÖ Gr√°fico guardado: model_comparison_roc_auc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Gr√°fico 3: Curvas ROC\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink']\n",
    "\n",
    "for idx, (model_name, result) in enumerate(results.items()):\n",
    "    if result['y_test_proba'] is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, result['y_test_proba'])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.plot(fpr, tpr, color=colors[idx % len(colors)], lw=2,\n",
    "                label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random (AUC = 0.500)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('Curvas ROC - Comparaci√≥n de Modelos', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../roc_curves_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print('‚úÖ Gr√°fico guardado: roc_curves_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Gr√°fico 4: Matrices de confusi√≥n\n",
    "n_models = len(results)\n",
    "n_cols = 3\n",
    "n_rows = (n_models + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5*n_rows))\n",
    "axes = axes.ravel() if n_models > 1 else [axes]\n",
    "\n",
    "for idx, (model_name, result) in enumerate(results.items()):\n",
    "    cm = confusion_matrix(y_test, result['y_test_pred'])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "               cbar=False, square=True, linewidths=1, linecolor='black')\n",
    "    axes[idx].set_title(f'{model_name}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Real')\n",
    "    axes[idx].set_xlabel('Predicho')\n",
    "    axes[idx].set_xticklabels(['No (0)', 'S√≠ (1)'])\n",
    "    axes[idx].set_yticklabels(['No (0)', 'S√≠ (1)'])\n",
    "\n",
    "# Ocultar ejes sobrantes\n",
    "for idx in range(n_models, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "print('‚úÖ Gr√°fico guardado: confusion_matrices.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Gr√°fico 5: Feature Importance (solo para modelos que lo soporten)\n",
    "if hasattr(best_result['model'], 'feature_importances_'):\n",
    "    importances = best_result['model'].feature_importances_\n",
    "    \n",
    "    # Obtener nombres de features si est√°n disponibles\n",
    "    try:\n",
    "        # Intentar cargar desde archivo CSV\n",
    "        feature_names = list(pd.read_csv('../../X_train_transformed.csv', nrows=0).columns)\n",
    "    except:\n",
    "        feature_names = [f'Feature_{i}' for i in range(len(importances))]\n",
    "    \n",
    "    # Top 20 features\n",
    "    top_n = 20\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False).head(top_n)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(len(feature_importance_df)), feature_importance_df['importance'], color='teal')\n",
    "    plt.yticks(range(len(feature_importance_df)), feature_importance_df['feature'])\n",
    "    plt.xlabel('Importancia', fontsize=12)\n",
    "    plt.title(f'Top {top_n} Features M√°s Importantes - {best_model_name}', \n",
    "             fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../../feature_importance_{best_model_name.replace(\" \", \"_\")}.png', \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    print(f'‚úÖ Gr√°fico guardado: feature_importance_{best_model_name.replace(\" \", \"_\")}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'\\nTop {top_n} Features:')\n",
    "    print(feature_importance_df.to_string(index=False))\n",
    "else:\n",
    "    print('‚ö†Ô∏è El modelo seleccionado no soporta feature importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. GUARDAR MEJOR MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print('\\n' + '='*80)\n",
    "print('GUARDAR MEJOR MODELO')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Guardar mejor modelo con metadata\n",
    "best_model = best_result['model']\n",
    "best_metrics = best_result['test_metrics']\n",
    "\n",
    "# Cargar preprocessor si est√° disponible\n",
    "try:\n",
    "    preprocessor = joblib.load('../../preprocessor.pkl')\n",
    "except:\n",
    "    preprocessor = None\n",
    "\n",
    "# Obtener nombres de features\n",
    "try:\n",
    "    feature_names = list(pd.read_csv('../../X_train_transformed.csv', nrows=0).columns)\n",
    "except:\n",
    "    feature_names = None\n",
    "\n",
    "model_package = {\n",
    "    'model': best_model,\n",
    "    'model_name': best_model_name,\n",
    "    'metrics': best_metrics,\n",
    "    'preprocessor': preprocessor,\n",
    "    'feature_names': feature_names,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'version': '1.0'\n",
    "}\n",
    "\n",
    "filename = '../../best_model.pkl'\n",
    "joblib.dump(model_package, filename)\n",
    "\n",
    "print(f'\\n‚úÖ Modelo guardado: {filename}')\n",
    "print(f'   Modelo: {best_model_name}')\n",
    "print(f'   F1-Score: {best_metrics[\"f1\"]:.4f}')\n",
    "print(f'   ROC-AUC: {best_metrics.get(\"roc_auc\", \"N/A\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. RESUMEN FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print('\\n' + '='*80)\n",
    "print('RESUMEN FINAL - FASE 3')\n",
    "print('='*80)\n",
    "\n",
    "print('\\n‚úÖ FASE 3 COMPLETADA EXITOSAMENTE')\n",
    "print(f'\\nüìä Resumen:')\n",
    "print(f'  1. Modelos entrenados: {len(results)}')\n",
    "print(f'  2. Mejor modelo: {best_model_name}')\n",
    "print(f'  3. F1-Score: {best_result[\"test_metrics\"][\"f1\"]:.4f}')\n",
    "print(f'  4. ROC-AUC: {best_result[\"test_metrics\"].get(\"roc_auc\", \"N/A\")}')\n",
    "print(f'  5. Accuracy: {best_result[\"test_metrics\"][\"accuracy\"]:.4f}')\n",
    "\n",
    "print('\\nüìÅ Archivos generados:')\n",
    "print('  - best_model.pkl')\n",
    "print('  - model_comparison_metrics.png')\n",
    "print('  - model_comparison_roc_auc.png')\n",
    "print('  - roc_curves_comparison.png')\n",
    "print('  - confusion_matrices.png')\n",
    "if hasattr(best_result['model'], 'feature_importances_'):\n",
    "    print(f'  - feature_importance_{best_model_name.replace(\" \", \"_\")}.png')\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('üéâ FASE 3 COMPLETADA - LISTO PARA FASE 4')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. VERIFICACI√ìN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verificar que el modelo se puede cargar\n",
    "try:\n",
    "    loaded_model = joblib.load('../../best_model.pkl')\n",
    "    print('‚úÖ Modelo cargado correctamente')\n",
    "    print(f'   Modelo: {loaded_model[\"model_name\"]}')\n",
    "    print(f'   F1-Score: {loaded_model[\"metrics\"][\"f1\"]:.4f}')\n",
    "    print(f'   Timestamp: {loaded_model[\"timestamp\"]}')\n",
    "except Exception as e:\n",
    "    print(f'‚ùå Error al cargar modelo: {str(e)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}